{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/composable_vlms/blob/main/notebooks/finetune_florence_2_large_liver_disease_40_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Florence-2 on Object Detection Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/florence-2/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)\n",
        "\n",
        "Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. The model demonstrates strong zero-shot and fine-tuning capabilities across tasks such as captioning, object detection, grounding, and segmentation.\n",
        "\n",
        "![Florence-2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-1.png)\n",
        "\n",
        "*Figure 1. Illustration showing the level of spatial hierarchy and semantic granularity expressed by each task. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n",
        "The model takes images and task prompts as input, generating the desired results in text format. It uses a DaViT vision encoder to convert images into visual token embeddings. These are then concatenated with BERT-generated text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response.\n",
        "\n",
        "![Florence-2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-2.png)\n",
        "\n",
        "*Figure 2. Overview of Florence-2 architecture. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n"
      ],
      "metadata": {
        "id": "KVmYbqqUSlCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "z16cfHRE8yi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_vp9cS2-gXbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ],
      "metadata": {
        "id": "mqd30Ndg9dbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `L4 GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "n32nrwCeAEYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "rMmBuhiiC2mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "**NOTE:** Feel free to replace our example image with your own photo."
      ],
      "metadata": {
        "id": "dOshHQM3Ebq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!ls -lh"
      ],
      "metadata": {
        "id": "u3ZhBCTPEnEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLE_IMAGE_PATH = \"dog.jpeg\""
      ],
      "metadata": {
        "id": "PbglpBOOFCHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and configure the model\n",
        "\n",
        " Let's download the model checkpoint and configure it so that you can fine-tune it later on."
      ],
      "metadata": {
        "id": "GM4QlaUfCFsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers flash_attn timm einops peft huggingface_hub\n",
        "!pip install -q roboflow git+https://github.com/roboflow/supervision.git"
      ],
      "metadata": {
        "id": "Y6b1dvjgYXOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bc3RpqBejH4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import html\n",
        "import base64\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from google.colab import userdata\n",
        "from IPython.core.display import display, HTML\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    get_scheduler\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any, Tuple, Generator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "from roboflow import Roboflow"
      ],
      "metadata": {
        "id": "HMd6tb4sSh9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor` classes from the transformers library. Note that you need to pass `trust_remote_code` as `True` since this model is not a standard transformers model."
      ],
      "metadata": {
        "id": "flp13B-8Myjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT = \"microsoft/Florence-2-large-ft\"\n",
        "# REVISION = 'refs/pr/6'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\n",
        "processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "zqDWEWDcaSxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference with pre-trained Florence-2 model"
      ],
      "metadata": {
        "id": "rf1GlvvQFec-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example object detection inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "ReAKWNxAFmv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example image captioning inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<DETAILED_CAPTION>\"\n",
        "text = \"<DETAILED_CAPTION>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "response"
      ],
      "metadata": {
        "id": "eSLSErXeI84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example caption to phrase grounding inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
        "text = \"<CAPTION_TO_PHRASE_GROUNDING> In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "PrVSTmFPJjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Florence-2 on custom dataset"
      ],
      "metadata": {
        "id": "eQetrQM7Jziy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset from Roboflow Universe"
      ],
      "metadata": {
        "id": "Sw7D6ZYzAs9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "# project = rf.workspace(\"roboflow-100\").project(\"bccd-ouzjz\")\n",
        "# version = project.version(2)\n",
        "# dataset = version.download(\"florence2-od\")\n",
        "\n",
        "project = rf.workspace(\"roboflow-100\").project(\"liver-disease\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"florence2-od\")\n"
      ],
      "metadata": {
        "id": "K1IlyjYmBCxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 {dataset.location}/train/annotations.jsonl"
      ],
      "metadata": {
        "id": "iiLclUnKTrLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define `DetectionsDataset` class\n",
        "\n",
        "class JSONLDataset:\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.jsonl_file_path = jsonl_file_path\n",
        "        self.image_directory_path = image_directory_path\n",
        "        self.entries = self._load_entries()\n",
        "\n",
        "    def _load_entries(self) -> List[Dict[str, Any]]:\n",
        "        entries = []\n",
        "        with open(self.jsonl_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data = json.loads(line)\n",
        "                entries.append(data)\n",
        "        return entries\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:\n",
        "        if idx < 0 or idx >= len(self.entries):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
        "        try:\n",
        "            image = Image.open(image_path)\n",
        "            return (image, entry)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n",
        "\n",
        "\n",
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, data = self.dataset[idx]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        return prefix, suffix, image"
      ],
      "metadata": {
        "id": "dExvJNFkxymc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    return inputs, answers\n",
        "\n",
        "train_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/train/\"\n",
        ")\n",
        "val_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/valid/\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "ilMb0ivGdt9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup LoRA Florence-2 model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    inference_mode=False,\n",
        "    use_rslora=True,\n",
        "    init_lora_weights=\"gaussian\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "FmPJOXCzB-29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1V9BcVQMycgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference with pre-trained Florence-2 model on validation dataset\n",
        "\n",
        "def render_inline(image: Image.Image, resize=(128, 128)):\n",
        "    \"\"\"Convert image into inline html.\"\"\"\n",
        "    image.resize(resize)\n",
        "    with io.BytesIO() as buffer:\n",
        "        image.save(buffer, format='jpeg')\n",
        "        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
        "        return f\"data:image/jpeg;base64,{image_b64}\"\n",
        "\n",
        "\n",
        "def render_example(image: Image.Image, response):\n",
        "    try:\n",
        "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n",
        "        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n",
        "    except:\n",
        "        print('failed to redner model response')\n",
        "    return f\"\"\"\n",
        "<div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" />\n",
        "    <p style=\"width:512px; margin:10px; font-size:small;\">{html.escape(json.dumps(response))}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def render_inference_results(model, dataset: DetectionDataset, count: int):\n",
        "    html_out = \"\"\n",
        "    count = min(count, len(dataset))\n",
        "    for i in range(count):\n",
        "        image, data = dataset.dataset[i]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3\n",
        "        )\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "        answer = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "        html_out += render_example(image, answer)\n",
        "\n",
        "    display(HTML(html_out))\n",
        "\n",
        "render_inference_results(peft_model, val_dataset, 4)"
      ],
      "metadata": {
        "id": "i9LEEXRwN9cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Florence-2 on custom object detection dataset"
      ],
      "metadata": {
        "id": "RH9JTq_RytE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define train loop\n",
        "\n",
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(\n",
        "                text=answers,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                return_token_type_ids=False\n",
        "            ).input_ids.to(DEVICE)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(\n",
        "                    text=answers,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    return_token_type_ids=False\n",
        "                ).input_ids.to(DEVICE)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "            render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "bC06Mc7jOdpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run train loop\n",
        "\n",
        "%%time\n",
        "\n",
        "EPOCHS = 40\n",
        "LR = 5e-6\n",
        "\n",
        "train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)"
      ],
      "metadata": {
        "id": "LZybGHd3fNJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuned model evaluation"
      ],
      "metadata": {
        "id": "MBHMu7WGWpeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check if the model can still detect objects outside of the custom dataset\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = peft_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "xspOKfgRY2gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** It seems that the model can still detect classes that don't belong to our custom dataset."
      ],
      "metadata": {
        "id": "3Ygh9VyZaqnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Collect predictions\n",
        "\n",
        "PATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)<loc_\\d+>'\n",
        "\n",
        "def extract_classes(dataset: DetectionDataset):\n",
        "    class_set = set()\n",
        "    for i in range(len(dataset.dataset)):\n",
        "        image, data = dataset.dataset[i]\n",
        "        suffix = data[\"suffix\"]\n",
        "        classes = re.findall(PATTERN, suffix)\n",
        "        class_set.update(classes)\n",
        "    return sorted(class_set)\n",
        "\n",
        "CLASSES = extract_classes(train_dataset)\n",
        "\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(val_dataset.dataset)):\n",
        "    image, data = val_dataset.dataset[i]\n",
        "    prefix = data['prefix']\n",
        "    suffix = data['suffix']\n",
        "\n",
        "    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    prediction = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n",
        "    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n",
        "    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n",
        "    prediction.confidence = np.ones(len(prediction))\n",
        "\n",
        "    target = processor.post_process_generation(suffix, task='<OD>', image_size=image.size)\n",
        "    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n",
        "    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n",
        "\n",
        "    targets.append(target)\n",
        "    predictions.append(prediction)"
      ],
      "metadata": {
        "id": "8f1BYeQw3xhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES"
      ],
      "metadata": {
        "id": "nKECYHh-z95f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate mAP\n",
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        ")\n",
        "\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ],
      "metadata": {
        "id": "88VnIZ_feHPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate Confusion Matrix\n",
        "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "_ = confusion_matrix.plot()"
      ],
      "metadata": {
        "id": "85APzNRfe8xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save fine-tuned model on hard drive"
      ],
      "metadata": {
        "id": "8rR2naNXzEB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.save_pretrained(\"/content/florence2-large-ft\")\n",
        "processor.save_pretrained(\"/content/florence2-large-ft/\")\n",
        "!ls -la /content/florence2-large/"
      ],
      "metadata": {
        "id": "Rdbmcv3TcIe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model to the Hub with your desired name\n",
        "peft_model.push_to_hub(\"dwb2023/florence2-large-liver-disease-ft\")\n",
        "processor.push_to_hub(\"dwb2023/florence2-large-liver-disease-ft\")"
      ],
      "metadata": {
        "id": "fyP9ZW2bf1te"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}