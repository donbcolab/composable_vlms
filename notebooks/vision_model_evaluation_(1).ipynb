{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/composable_vlms/blob/main/notebooks/vision_model_evaluation_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "!pip install -q pycocotools Pillow tqdm transformers torch datasets huggingface_hub wandb timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYX5vRvTvcxg",
        "outputId": "c7f975d1-9a8b-49d4-8f68-1b9bddcf3973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "# setting the WANDB_API_KEY environment variable\n",
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "2aW9uSHy0ClP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import requests\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pycocotools.coco import COCO\n",
        "from datasets import load_dataset, Dataset\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"annotation_file\": '/content/annotations/instances_train2017.json',\n",
        "    \"image_directory\": '/content/train2017/',\n",
        "    \"num_images\": 500,\n",
        "    \"hf_dataset_name\": \"dwb2023/coco-subset-for-vision-evaluation\",\n",
        "    \"coco_annotations_url\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "    \"coco_images_url\": \"http://images.cocodataset.org/zips/train2017.zip\"\n",
        "}\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n"
      ],
      "metadata": {
        "id": "DmmCWJm61HOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url: str, save_path: str):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024\n",
        "    with open(save_path, 'wb') as file, tqdm(\n",
        "        desc=save_path,\n",
        "        total=total_size,\n",
        "        unit='iB',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as progress_bar:\n",
        "        for data in response.iter_content(block_size):\n",
        "            size = file.write(data)\n",
        "            progress_bar.update(size)\n",
        "\n",
        "def download_coco_data():\n",
        "    import zipfile\n",
        "\n",
        "    # Download annotations\n",
        "    os.makedirs('/content/annotations', exist_ok=True)\n",
        "    annotations_zip = '/content/annotations.zip'\n",
        "    logging.info(\"Downloading COCO annotations...\")\n",
        "    download_file(CONFIG['coco_annotations_url'], annotations_zip)\n",
        "\n",
        "    # Extract annotations\n",
        "    logging.info(\"Extracting COCO annotations...\")\n",
        "    with zipfile.ZipFile(annotations_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    os.remove(annotations_zip)\n",
        "\n",
        "    # Download images\n",
        "    os.makedirs(CONFIG['image_directory'], exist_ok=True)\n",
        "    images_zip = '/content/train2017.zip'\n",
        "    logging.info(\"Downloading COCO images...\")\n",
        "    download_file(CONFIG['coco_images_url'], images_zip)\n",
        "\n",
        "    # Extract images\n",
        "    logging.info(\"Extracting COCO images...\")\n",
        "    with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    os.remove(images_zip)\n",
        "\n",
        "    logging.info(\"COCO data downloaded and extracted successfully\")"
      ],
      "metadata": {
        "id": "IzBRrCWU1YJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data() -> Tuple[List[Dict[str, Any]], COCO]:\n",
        "    if not os.path.exists(CONFIG[\"annotation_file\"]):\n",
        "        logging.info(\"COCO annotations not found. Downloading COCO data...\")\n",
        "        download_coco_data()\n",
        "\n",
        "    coco = COCO(CONFIG[\"annotation_file\"])\n",
        "    catIds = coco.getCatIds(catNms=['person', 'car'])\n",
        "    imgIds = coco.getImgIds(catIds=catIds)\n",
        "    images = coco.loadImgs(imgIds[:CONFIG[\"num_images\"]])\n",
        "    return images, coco\n",
        "\n",
        "def prepare_and_upload_hf_dataset(images: List[Dict[str, Any]], coco: COCO, hf_dataset_name: str) -> Dataset:\n",
        "    dataset_dict = {\n",
        "        \"image_id\": [],\n",
        "        \"file_name\": [],\n",
        "        \"width\": [],\n",
        "        \"height\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "    for img in tqdm(images, desc=\"Preparing dataset\"):\n",
        "        dataset_dict[\"image_id\"].append(img['id'])\n",
        "        dataset_dict[\"file_name\"].append(img['file_name'])\n",
        "        dataset_dict[\"width\"].append(img['width'])\n",
        "        dataset_dict[\"height\"].append(img['height'])\n",
        "        dataset_dict[\"annotations\"].append(coco.loadAnns(coco.getAnnIds(imgIds=img['id'])))\n",
        "\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    try:\n",
        "        dataset.push_to_hub(hf_dataset_name)\n",
        "        logging.info(f\"Dataset pushed to Hugging Face: {hf_dataset_name}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to push dataset to Hugging Face: {e}\")\n",
        "        logging.info(\"Continuing without uploading to Hugging Face\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "uzaGj2P21iRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    logging.info(\"Starting Vision Model Evaluation Setup\")\n",
        "\n",
        "    # Try to load existing dataset from Hugging Face\n",
        "    try:\n",
        "        dataset = load_dataset(CONFIG[\"hf_dataset_name\"])\n",
        "        images = dataset['train']\n",
        "        coco = COCO(CONFIG[\"annotation_file\"])\n",
        "        logging.info(f\"Loaded dataset from Hugging Face: {CONFIG['hf_dataset_name']}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Failed to load Hugging Face dataset: {e}\")\n",
        "        logging.info(\"Starting data preparation\")\n",
        "        images, coco = prepare_data()\n",
        "        dataset = prepare_and_upload_hf_dataset(images, coco, CONFIG[\"hf_dataset_name\"])\n",
        "\n",
        "    logging.info(\"Data preparation and loading complete\")\n",
        "    return dataset, coco\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset, coco = main()\n",
        "    print(\"Dataset sample:\", dataset[0])\n",
        "    print(\"Number of images:\", len(dataset))"
      ],
      "metadata": {
        "id": "c2GMzguz1NT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}