{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/composable_vlms/blob/main/notebooks/weave_intro_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298501c0",
      "metadata": {
        "id": "298501c0"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{intro-colab} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974b2c96",
      "metadata": {
        "id": "974b2c96"
      },
      "source": [
        "# üèÉ‚Äç‚ôÄÔ∏è Quickstart\n",
        "\n",
        "Get started using Weave to:\n",
        "- Log and debug language model inputs, outputs, and traces\n",
        "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
        "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
        "\n",
        "See the full Weave documentation [here](https://wandb.me/weave).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dc5c9d",
      "metadata": {
        "id": "56dc5c9d"
      },
      "source": [
        "## ü™Ñ Install `weave` library and login\n",
        "\n",
        "\n",
        "Start by installing the library and logging in to your account.\n",
        "\n",
        "In this example, we're using openai so you should [add an openai API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "61a6d209",
      "metadata": {
        "id": "61a6d209"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install weave openai set-env-colab-kaggle-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0921bbef-d089-4960-b624-505172f90d76",
      "metadata": {
        "id": "0921bbef-d089-4960-b624-505172f90d76"
      },
      "outputs": [],
      "source": [
        "# Set your OpenAI API key\n",
        "from set_env import set_env\n",
        "\n",
        "# Put your OPENAI_API_KEY in the secrets panel to the left üóùÔ∏è\n",
        "_ = set_env(\"OPENAI_API_KEY\")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # alternatively, put your key here\n",
        "\n",
        "PROJECT = \"weave-intro-notebook\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09467a33",
      "metadata": {
        "id": "09467a33"
      },
      "source": [
        "# Track inputs & outputs of functions\n",
        "\n",
        "Weave allows users to track function calls: the code, inputs, outputs, and even LLM tokens & costs! In the following sections we will cover:\n",
        "\n",
        "* Custom Functions\n",
        "* Vendor Integrations\n",
        "* Nested Function Calling\n",
        "* Error Tracking\n",
        "\n",
        "Note: in all cases, we will:\n",
        "\n",
        "```python\n",
        "import weave                    # import the weave library\n",
        "weave.init('project-name')      # initialize tracking for a specific W&B project\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99453d48",
      "metadata": {
        "id": "99453d48"
      },
      "source": [
        "## Track custom functions\n",
        "\n",
        "Add the @weave.op decorator to the functions you want to track"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "Qy9GnIAGYFG9"
      },
      "id": "Qy9GnIAGYFG9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cdcd1bb7",
      "metadata": {
        "id": "cdcd1bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f92f085-0421-427a-fd6e-87575799ec53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/a81cb5b7-b8e5-45fd-8304-1c2860fe316a\n",
            "That was so easy; it was a piece of cake!\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "client = OpenAI()\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a grammar checker, correct the following user input.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"That was so easy, it was a piece of pie!\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "generation = response.choices[0].message.content\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174cabef",
      "metadata": {
        "id": "174cabef"
      },
      "source": [
        "You can find your interactive dashboard by clicking any of the  üëÜ wandb links above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef6f27b",
      "metadata": {
        "id": "5ef6f27b"
      },
      "source": [
        "## Vendor Integrations (OpenAI, Anthropic, Mistral, etc...)\n",
        "\n",
        "Here, we're automatically tracking all calls to `openai`. We automatically track a lot of LLM libraries, but it's really easy to add support for whatever LLM you're using, as you'll see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ac8a2b59",
      "metadata": {
        "id": "ac8a2b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5e06cd-ecc0-41d1-bc42-9792d73c92ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/2adb4e8f-1005-46ca-b6bb-0de06cb27e9e\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def strip_user_input(user_input):\n",
        "    return user_input.strip()\n",
        "\n",
        "\n",
        "result = strip_user_input(\"    hello    \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed90e37",
      "metadata": {
        "id": "1ed90e37"
      },
      "source": [
        "After adding `weave.op` and calling the function, visit the link and see it tracked within your project.\n",
        "\n",
        "üí° We automatically track your code, have a look at the code tab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35fe43dc",
      "metadata": {
        "id": "35fe43dc"
      },
      "source": [
        "## Track nested functions\n",
        "\n",
        "Now that you've seen the basics, let's combine all of the above and track some deeply nested functions alongside LLM calls.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "422050d0",
      "metadata": {
        "id": "422050d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4d92dc-1b53-401a-bf02-ed4b7db0eac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/94c97f24-22c8-422f-9090-9900f953cede\n",
            "That was so easy; it was a piece of cake!\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def strip_user_input(user_input):\n",
        "    return user_input.strip()\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def correct_grammar(user_input):\n",
        "    client = OpenAI()\n",
        "\n",
        "    stripped = strip_user_input(user_input)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a grammar checker, correct the following user input.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": stripped},\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f483413",
      "metadata": {
        "id": "6f483413"
      },
      "source": [
        "## Track Errors\n",
        "\n",
        "Whenever your code crashes, weave will highlight what caused the issue. This is especially useful for finding things like JSON parsing issues that can occasionally happen when parsing data from LLM responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "14e7e103",
      "metadata": {
        "id": "14e7e103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2938eff3-0ce8-40ca-d61b-71d738cdaff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/beefae30-f622-4718-9e45-ba1de8f2c053\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-909d17503770>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_grammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   That was so easy, it was a piece of pie!    \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     return _execute_call(\n\u001b[0m\u001b[1;32m    331\u001b[0m                         \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                         \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_execute_call\u001b[0;34m(__op, call, __return_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-909d17503770>\u001b[0m in \u001b[0;36mcorrect_grammar\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstripped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     return _execute_call(\n\u001b[0m\u001b[1;32m    331\u001b[0m                         \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                         \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_execute_call\u001b[0;34m(__op, call, __return_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/weave/integrations/openai/openai_sdk.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 return fn(\n\u001b[0m\u001b[1;32m    248\u001b[0m                     \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 )  # This is where the final execution of fn is happening.\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def strip_user_input(user_input):\n",
        "    return user_input.strip()\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "def correct_grammar(user_input):\n",
        "    client = OpenAI()\n",
        "\n",
        "    stripped = strip_user_input(user_input)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a grammar checker, correct the following user input.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": stripped},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f957c9c8",
      "metadata": {
        "id": "f957c9c8"
      },
      "source": [
        "# Tracking Objects\n",
        "\n",
        "Organizing experimentation is difficult when there are many moving pieces. You can capture and organize the experimental details of your app like your system prompt or the model you're using within `weave.Objects`. This helps organize and compare different iterations of your app. In this section, we will cover:\n",
        "\n",
        "* General Object Tracking\n",
        "* Tracking Models\n",
        "* Tracking Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6129cc",
      "metadata": {
        "id": "8f6129cc"
      },
      "source": [
        "## General Object Tracking\n",
        "\n",
        "Many times, it is useful to track & version data, just like you track and version code. For example, here we define a `SystemPrompt(weave.Object)` object that can be shared between teammates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "08e4e852",
      "metadata": {
        "id": "08e4e852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982cb30a-9770-4235-d9ab-3b120619241c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Published to https://wandb.ai/dwbranson/weave-intro-notebook/weave/objects/SystemPrompt/versions/C2NtZFmQa6zY3l7YW4ZrpSI8tQCrwLogfHYYJw8OWDg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectRef(entity='dwbranson', project='weave-intro-notebook', name='SystemPrompt', digest='C2NtZFmQa6zY3l7YW4ZrpSI8tQCrwLogfHYYJw8OWDg', extra=())"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "\n",
        "class SystemPrompt(weave.Object):\n",
        "    prompt: str\n",
        "\n",
        "\n",
        "system_prompt = SystemPrompt(\n",
        "    prompt=\"You are a grammar checker, correct the following user input.\"\n",
        ")\n",
        "weave.publish(system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c873438",
      "metadata": {
        "id": "0c873438"
      },
      "source": [
        "## Model Tracking\n",
        "\n",
        "Models are so common of an object type, that we have a special class to represent them: `weave.Model`. The only requirement is that we define a `predict` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e3e60d81",
      "metadata": {
        "id": "e3e60d81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c32d8b7-e8a9-446a-92eb-d3e6d1ad464f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/7d8a1af2-f346-489b-a8c3-abbbf90bb59b\n",
            "That was so easy; it was a piece of cake!\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "\n",
        "class OpenAIGrammarCorrector(weave.Model):\n",
        "    # Properties are entirely user-defined\n",
        "    openai_model_name: str\n",
        "    system_message: str\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, user_input):\n",
        "        client = OpenAI()\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.openai_model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.system_message},\n",
        "                {\"role\": \"user\", \"content\": user_input},\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "corrector = OpenAIGrammarCorrector(\n",
        "    openai_model_name=\"gpt-4o\",\n",
        "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
        ")\n",
        "\n",
        "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624b04c1",
      "metadata": {
        "id": "624b04c1"
      },
      "source": [
        "## Dataset Tracking\n",
        "\n",
        "Similar to models, a `weave.Dataset` object exists to help track, organize, and operate on datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ac2c5145",
      "metadata": {
        "id": "ac2c5145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66ab5b2-9320-4743-c19e-895460e5f4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Published to https://wandb.ai/dwbranson/weave-intro-notebook/weave/objects/grammar-correction/versions/uWYy3cibtrEAvbZsXcBavWiqtqi5vtne87VZl6WmZyE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectRef(entity='dwbranson', project='weave-intro-notebook', name='grammar-correction', digest='uWYy3cibtrEAvbZsXcBavWiqtqi5vtne87VZl6WmZyE', extra=())"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset = weave.Dataset(\n",
        "    name=\"grammar-correction\",\n",
        "    rows=[\n",
        "        {\n",
        "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
        "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
        "        },\n",
        "        {\"user_input\": \"  I write good   \", \"expected\": \"I write well\"},\n",
        "        {\n",
        "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
        "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "\n",
        "weave.publish(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d43bc3",
      "metadata": {
        "id": "f7d43bc3"
      },
      "source": [
        "Notice that we saved a versioned `GrammarCorrector` object that captures the configurations you're experimenting with."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c3aaea",
      "metadata": {
        "id": "71c3aaea"
      },
      "source": [
        "## Retrieve Published Objects & Ops\n",
        "\n",
        "You can publish objects and then retrieve them in your code. You can even call functions from your retrieved objects!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "caa5c5e0",
      "metadata": {
        "id": "caa5c5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df95d2a-655a-420f-9b5d-cf6e71847d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Published to https://wandb.ai/dwbranson/weave-intro-notebook/weave/objects/OpenAIGrammarCorrector/versions/rGXnZ5fwBSwtmevqamqXcHiapOq0OLy0sESsMU79z1Y\n",
            "weave:///dwbranson/weave-intro-notebook/object/OpenAIGrammarCorrector:rGXnZ5fwBSwtmevqamqXcHiapOq0OLy0sESsMU79z1Y\n"
          ]
        }
      ],
      "source": [
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "corrector = OpenAIGrammarCorrector(\n",
        "    openai_model_name=\"gpt-4o\",\n",
        "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
        ")\n",
        "\n",
        "ref = weave.publish(corrector)\n",
        "print(ref.uri())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9ff9cfe7",
      "metadata": {
        "id": "9ff9cfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebec159-a7b2-489c-bfe9-220a8c445521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/dwbranson/weave-intro-notebook/r/call/a99b9f6e-8ef0-4bd3-9f10-f8ae6547f839\n",
            "That was so easy; it was a piece of cake!\n"
          ]
        }
      ],
      "source": [
        "import weave\n",
        "\n",
        "weave.init(PROJECT)\n",
        "\n",
        "# Note: this url is available from the UI after publishing the object!\n",
        "ref_url = f\"weave:///{ref.entity}/{PROJECT}/object/{ref.name}:{ref.digest}\"\n",
        "fetched_collector = weave.ref(ref_url).get()\n",
        "\n",
        "# Notice: this object was loaded from remote location!\n",
        "result = fetched_collector.predict(\"That was so easy, it was a piece of pie!\")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b94dd5",
      "metadata": {
        "id": "77b94dd5"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Evaluation-driven development helps you reliably iterate on an application. The `Evaluation` class is designed to assess the performance of a `Model` on a given `Dataset` or set of examples using scoring functions.\n",
        "\n",
        "See a preview of the API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c597f9a",
      "metadata": {
        "id": "9c597f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0d659c7c-778a-483b-e705-61dfb2df3a2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation summary\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'exact_match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.3333333333333333\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.3613890012105307\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'exact_match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3333333333333333</span><span style=\"font-weight: bold\">}}</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3613890012105307</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import weave\n",
        "from weave import Evaluation\n",
        "\n",
        "\n",
        "# Define any custom scoring function\n",
        "@weave.op()\n",
        "def exact_match(expected: str, model_output: dict) -> dict:\n",
        "    # Here is where you'd define the logic to score the model output\n",
        "    return {\"match\": expected == model_output}\n",
        "\n",
        "\n",
        "# Score your examples using scoring functions\n",
        "evaluation = Evaluation(\n",
        "    dataset=dataset,  # can be a list of dictionaries or a weave.Dataset object\n",
        "    scorers=[exact_match],  # can be a list of scoring functions\n",
        ")\n",
        "\n",
        "# Start tracking the evaluation\n",
        "weave.init(PROJECT)\n",
        "# Run the evaluation\n",
        "summary = await evaluation.evaluate(corrector)  # can be a model or simple function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d997d9e6",
      "metadata": {
        "id": "d997d9e6"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "Follow the [Build an Evaluation pipeline](http://wandb.me/weave_eval_tut) tutorial to learn more about Evaluation and begin iteratively improving your applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}