{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNIML817m8prLC0ArIbtJ7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/composable_vlms/blob/main/notebooks/vision_model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yNTQMkozvIE"
      },
      "outputs": [],
      "source": [
        "!pip install -q pycocotools Pillow tqdm transformers torch wandb timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "# setting the WANDB_API_KEY environment variable\n",
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "2aW9uSHy0ClP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1: List the contents of the current directory\n",
        "# print(\"Listing directory contents:\")\n",
        "# !ls\n",
        "\n",
        "# # Step 2: Verify the file name and ensure it matches in the command\n",
        "# print(\"Running vision-model-evaluation.py:\")\n",
        "# !python vision-model-evaluation.py\n"
      ],
      "metadata": {
        "id": "qy_dNd7q1Ukq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 1: Imports and Configuration"
      ],
      "metadata": {
        "id": "fmtd0D5xM4ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import zipfile\n",
        "import logging\n",
        "import torch\n",
        "import wandb\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "from transformers import AutoModelForObjectDetection, AutoModelForImageSegmentation, AutoProcessor\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='model_evaluation.log', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "logging.getLogger().addHandler(console_handler)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"annotation_file\": '/content/annotations/instances_train2017.json',\n",
        "    \"image_directory\": '/content/train2017/',\n",
        "    \"num_images\": 500,\n",
        "    \"batch_size\": 8,  # Reduce batch size to manage GPU memory\n",
        "    \"detection_model_name\": \"facebook/detr-resnet-50\",\n",
        "    \"segmentation_model_name\": \"facebook/detr-resnet-50-panoptic\",\n",
        "}\n",
        "\n",
        "def load_image(image_info):\n",
        "    image_path = os.path.join(CONFIG['image_directory'], image_info['file_name'])\n",
        "    logging.info(f\"Loading image from {image_path}\")\n",
        "    return Image.open(image_path)\n"
      ],
      "metadata": {
        "id": "tbTBEq2yCn2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 2: Dataset Preparation"
      ],
      "metadata": {
        "id": "l3-DkZvKM7Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, dest_path):\n",
        "    logging.info(f\"Downloading from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error for bad status codes\n",
        "    with open(dest_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    logging.info(f\"Downloaded to {dest_path}\")\n",
        "\n",
        "def extract_zip(file_path, extract_to):\n",
        "    logging.info(f\"Extracting {file_path}...\")\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    os.remove(file_path)\n",
        "    logging.info(f\"Extracted to {extract_to}\")\n",
        "\n",
        "def download_coco_dataset():\n",
        "    annotation_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "    annotation_zip = \"/content/annotations.zip\"\n",
        "    download_file(annotation_url, annotation_zip)\n",
        "    extract_zip(annotation_zip, '/content/')\n",
        "\n",
        "    image_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
        "    image_zip = \"/content/train2017.zip\"\n",
        "    download_file(image_url, image_zip)\n",
        "    extract_zip(image_zip, '/content/')\n",
        "\n",
        "def prepare_data():\n",
        "    if not os.path.exists(CONFIG[\"annotation_file\"]):\n",
        "        download_coco_dataset()\n",
        "\n",
        "    coco = COCO(CONFIG[\"annotation_file\"])\n",
        "    catIds = coco.getCatIds(catNms=['person', 'car'])\n",
        "    imgIds = coco.getImgIds(catIds=catIds)\n",
        "    images = coco.loadImgs(imgIds[:CONFIG[\"num_images\"]])\n",
        "\n",
        "    return images, coco\n"
      ],
      "metadata": {
        "id": "munmiNevLdTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 3: Model Loading"
      ],
      "metadata": {
        "id": "4RufLOlXNAS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    try:\n",
        "        logging.info(\"Loading detection model...\")\n",
        "        detection_model = AutoModelForObjectDetection.from_pretrained(CONFIG[\"detection_model_name\"]).to(device)\n",
        "        detection_processor = AutoProcessor.from_pretrained(CONFIG[\"detection_model_name\"])\n",
        "\n",
        "        logging.info(\"Loading segmentation model...\")\n",
        "        segmentation_model = AutoModelForImageSegmentation.from_pretrained(CONFIG[\"segmentation_model_name\"]).to(device)\n",
        "        segmentation_processor = AutoProcessor.from_pretrained(CONFIG[\"segmentation_model_name\"])\n",
        "\n",
        "        return detection_model, detection_processor, segmentation_model, segmentation_processor\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading models: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "CGFcR2dMLfh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 4: Model Inference"
      ],
      "metadata": {
        "id": "zg2wu9kJNEK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_inference(model, processor, images, task=\"detection\"):\n",
        "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
        "    try:\n",
        "        outputs = model(**inputs)\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        logging.error(\"CUDA out of memory. Reduce the batch size and try again.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model inference: {e}\")\n",
        "        return None\n",
        "\n",
        "    if task == \"detection\":\n",
        "        results = processor.post_process_object_detection(outputs, target_sizes=[(img.height, img.width) for img in images])\n",
        "    elif task == \"segmentation\":\n",
        "        results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[(img.height, img.width) for img in images])\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "BdW_9yMiLmrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 5: Metrics Calculation"
      ],
      "metadata": {
        "id": "kMqfPv8VNKuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x3, y3, x4, y4 = box2\n",
        "\n",
        "    xi1, yi1 = max(x1, x3), max(y1, y3)\n",
        "    xi2, yi2 = min(x2, x4), min(y2, y4)\n",
        "\n",
        "    intersection = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x4 - x3) * (y4 - y3)\n",
        "\n",
        "    union = box1_area + box2_area - intersection\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def calculate_metrics(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    matches = []\n",
        "    for pred in pred_boxes:\n",
        "        match = any(calculate_iou(pred, gt) > iou_threshold for gt in gt_boxes)\n",
        "        matches.append(1 if match else 0)\n",
        "\n",
        "    precision = precision_score([1] * len(gt_boxes), matches, zero_division=0)\n",
        "    recall = recall_score([1] * len(gt_boxes), matches, zero_division=0)\n",
        "    f1 = f1_score([1] * len(gt_boxes), matches, zero_division=0)\n",
        "    ap = average_precision_score([1] * len(gt_boxes), matches)\n",
        "\n",
        "    return precision, recall, f1, ap\n"
      ],
      "metadata": {
        "id": "9Vx-ZnROLpPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 6: Results Visualization"
      ],
      "metadata": {
        "id": "rRsD2uxMNRGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(image, results, model_name, image_id):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for result in results:\n",
        "        if 'bbox' in result:\n",
        "            bbox = result['bbox']\n",
        "            draw.rectangle(bbox, outline=\"red\", width=2)\n",
        "            draw.text((bbox[0], bbox[1]), result['label'], fill=\"red\")\n",
        "\n",
        "    image_path = f\"{model_name}_result_{image_id}.jpg\"\n",
        "    try:\n",
        "        image.save(image_path)\n",
        "        logging.info(f\"Saved visualization: {image_path}\")\n",
        "    except IOError as e:\n",
        "        logging.error(f\"Error saving visualization: {e}\")\n",
        "\n",
        "    return image_path\n"
      ],
      "metadata": {
        "id": "En47BHu4Lt-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 7: Model Evaluation"
      ],
      "metadata": {
        "id": "zf1AhlNgNVn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch, model, processor, coco, task):\n",
        "    logging.info(f\"Processing batch of size {len(batch)} for task: {task}\")\n",
        "    batch_images = [load_image(img) for img in batch]\n",
        "    results = model_inference(model, processor, batch_images, task)\n",
        "    if results is None:\n",
        "        logging.error(\"Skipping batch due to CUDA out of memory error.\")\n",
        "        return []\n",
        "\n",
        "    batch_results = []\n",
        "    for img, result in zip(batch, results):\n",
        "        gt_boxes = [ann['bbox'] for ann in coco.loadAnns(coco.getAnnIds(imgIds=img['id']))]\n",
        "        pred_boxes = result['boxes'].tolist()\n",
        "        precision, recall, f1, ap = calculate_metrics(pred_boxes, gt_boxes)\n",
        "        vis_path = visualize_results(load_image(img), result, task, img['id'])\n",
        "        batch_results.append({\n",
        "            \"image_id\": img['id'],\n",
        "            \"model\": task,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"ap\": ap,\n",
        "            \"visualization\": vis_path\n",
        "        })\n",
        "    logging.info(f\"Completed processing batch for task: {task}\")\n",
        "    return batch_results\n",
        "\n",
        "def evaluate_models(images, coco, models):\n",
        "    detection_model, detection_processor, segmentation_model, segmentation_processor = models\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm(range(0, len(images), CONFIG[\"batch_size\"]), desc=\"Processing batches\"):\n",
        "        batch = images[i:i+CONFIG[\"batch_size\"]]\n",
        "\n",
        "        logging.info(f\"Running detection on batch {i//CONFIG['batch_size']+1}\")\n",
        "        results.extend(process_batch(batch, detection_model, detection_processor, coco, \"detection\"))\n",
        "\n",
        "        logging.info(f\"Running segmentation on batch {i//CONFIG['batch_size']+1}\")\n",
        "        results.extend(process_batch(batch, segmentation_model, segmentation_processor, coco, \"segmentation\"))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Xr2-aBSWLxHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 8: Main Function and JSON Validation"
      ],
      "metadata": {
        "id": "XanQKaJONcsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            json.load(f)\n",
        "        return True\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.error(f\"JSONDecodeError: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_prepared_data(flag_path, annotation_file):\n",
        "    with open(flag_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        images = data[\"images\"]\n",
        "        coco = COCO(annotation_file)\n",
        "    return images, coco\n",
        "\n",
        "def save_prepared_data(flag_path, images):\n",
        "    with open(flag_path, 'w') as f:\n",
        "        json.dump({\"images\": images}, f)"
      ],
      "metadata": {
        "id": "H1H2LWibLyPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    wandb.init(project=\"vision-models-evaluation\", config=CONFIG, resume=True)\n",
        "\n",
        "    data_prepared_flag = '/content/data_prepared.flag'\n",
        "    if os.path.exists(data_prepared_flag) and validate_json_file(data_prepared_flag):\n",
        "        logging.info(\"Data preparation already done, skipping...\")\n",
        "        images, coco = load_prepared_data(data_prepared_flag, CONFIG[\"annotation_file\"])\n",
        "    else:\n",
        "        logging.info(\"Starting data preparation\")\n",
        "        images, coco = prepare_data()\n",
        "        save_prepared_data(data_prepared_flag, images)\n",
        "\n",
        "    models_loaded_flag = '/content/models_loaded.flag'\n",
        "    if os.path.exists(models_loaded_flag) and validate_json_file(models_loaded_flag):\n",
        "        logging.info(\"Models already loaded, skipping...\")\n",
        "    else:\n",
        "        logging.info(\"Loading models\")\n",
        "        models = load_models()\n",
        "        with open(models_loaded_flag, 'w') as f:\n",
        "            json.dump({\"models_loaded\": True}, f)\n",
        "\n",
        "    logging.info(\"Starting model evaluation\")\n",
        "    results = evaluate_models(images, coco, models)\n",
        "\n",
        "    # Log results to wandb\n",
        "    table = wandb.Table(dataframe=pd.DataFrame(results))\n",
        "    wandb.log({\"results\": table})\n",
        "\n",
        "    for result in results:\n",
        "        wandb.log({f\"{result['model']}_visualization\": wandb.Image(result['visualization'])})\n",
        "\n",
        "    logging.info(\"Evaluation complete\")\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HhXaBzkOL4C2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}