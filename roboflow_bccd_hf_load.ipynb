{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl1WUYWtNE2Pdel8LMf8Gm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbcolab/composable_vlms/blob/main/roboflow_bccd_hf_load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q roboflow datasets datasets[vision]"
      ],
      "metadata": {
        "id": "JFMi8nxJsqct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from roboflow import Roboflow\n",
        "\n",
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "project = rf.workspace(\"roboflow-100\").project(\"bccd-ouzjz\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(model_format=\"coco\", location=\"/content/coco/\", overwrite=True)\n",
        "dataset = version.download(model_format=\"paligemma\", location=\"/content/paligemma/\", overwrite=True)\n",
        "dataset = version.download(model_format=\"florence2-od\", location=\"/content/florence2_od/\", overwrite=True)"
      ],
      "metadata": {
        "id": "cWyUczWctate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtzP2xV9sV52"
      },
      "outputs": [],
      "source": [
        "# for each of the dataset model formats (coco / paligemma / florence2_od)\n",
        "# do a \"ls -l\" of the json files\n",
        "!ls -l coco/train/*.json\n",
        "!ls -l coco/test/*.json\n",
        "!ls -l coco/valid/*.json\n",
        "\n",
        "!ls -l florence2_od/test/*.json*\n",
        "!ls -l florence2_od/train/*.json*\n",
        "!ls -l florence2_od/valid/*.json*\n",
        "\n",
        "!ls -l paligemma/dataset/*.json*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_jpg_files(directory):\n",
        "    return len([f for f in os.listdir(directory) if f.endswith('.jpg')])\n",
        "\n",
        "directories = [\n",
        "    \"/content/coco/train\",\n",
        "    \"/content/coco/test\",\n",
        "    \"/content/coco/valid\",\n",
        "    \"/content/florence2_od/test\",\n",
        "    \"/content/florence2_od/train\",\n",
        "    \"/content/florence2_od/valid\",\n",
        "    \"/content/paligemma/dataset\"\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    try:\n",
        "        count = count_jpg_files(directory)\n",
        "        print(f\"{directory}: {count} files\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{directory}: Directory not found\")\n"
      ],
      "metadata": {
        "id": "oSgcUrNPu6Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset, Features, Image, Sequence, Value, ClassLabel, DatasetDict\n",
        "\n",
        "def load_coco_annotations(annotation_file, image_dir):\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create id to filename mapping\n",
        "    id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n",
        "\n",
        "    # Create category id to name mapping\n",
        "    id_to_category = {cat['id']: cat['name'] for cat in data['categories']}\n",
        "\n",
        "    # Process annotations\n",
        "    image_annotations = {}\n",
        "    for ann in data['annotations']:\n",
        "        image_id = ann['image_id']\n",
        "        if image_id not in image_annotations:\n",
        "            image_annotations[image_id] = []\n",
        "        image_annotations[image_id].append(ann)\n",
        "\n",
        "    # Create dataset entries\n",
        "    dataset_entries = []\n",
        "    for img in data['images']:\n",
        "        entry = {\n",
        "            'image_id': img['id'],\n",
        "            'image': os.path.join(image_dir, img['file_name']),\n",
        "            'width': img['width'],\n",
        "            'height': img['height'],\n",
        "            'objects': {\n",
        "                'id': [],\n",
        "                'area': [],\n",
        "                'bbox': [],\n",
        "                'category': []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if img['id'] in image_annotations:\n",
        "            for ann in image_annotations[img['id']]:\n",
        "                entry['objects']['id'].append(ann['id'])\n",
        "                entry['objects']['area'].append(ann['area'])\n",
        "                entry['objects']['bbox'].append(ann['bbox'])\n",
        "                entry['objects']['category'].append(id_to_category[ann['category_id']])\n",
        "\n",
        "        dataset_entries.append(entry)\n",
        "\n",
        "    return dataset_entries\n",
        "\n",
        "# Load annotations\n",
        "train_data = load_coco_annotations('/content/coco/train/_annotations.coco.json', '/content/coco/train')\n",
        "test_data = load_coco_annotations('/content/coco/test/_annotations.coco.json', '/content/coco/test')\n",
        "valid_data = load_coco_annotations('/content/coco/valid/_annotations.coco.json', '/content/coco/valid')\n",
        "\n",
        "# Define features\n",
        "features = Features({\n",
        "    'image_id': Value('int64'),\n",
        "    'image': Image(),\n",
        "    'width': Value('int32'),\n",
        "    'height': Value('int32'),\n",
        "    'objects': {\n",
        "        'id': Sequence(Value('int64')),\n",
        "        'area': Sequence(Value('int64')),\n",
        "        'bbox': Sequence(Sequence(Value('float32'), length=4)),\n",
        "        'category': Sequence(ClassLabel(names=['RBC', 'WBC', 'Platelets']))\n",
        "    }\n",
        "})\n",
        "\n",
        "# Create datasets\n",
        "coco_train_dataset = Dataset.from_list(train_data, features=features)\n",
        "coco_test_dataset = Dataset.from_list(test_data, features=features)\n",
        "coco_valid_dataset = Dataset.from_list(valid_data, features=features)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "coco_ds = DatasetDict({\n",
        "    'train': coco_train_dataset,\n",
        "    'test': coco_test_dataset,\n",
        "    'validation': coco_valid_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "TwLUjd-LvRfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hub\n",
        "coco_ds.push_to_hub(\"dwb2023/bccd-coco\")\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(f\"Dataset splits: {coco_ds.keys()}\")\n",
        "print(f\"Number of training examples: {len(coco_ds['train'])}\")\n",
        "print(f\"Number of testing examples: {len(coco_ds['test'])}\")\n",
        "print(f\"Number of validation examples: {len(coco_ds['validation'])}\")\n",
        "print(f\"Features: {coco_ds['train'].features}\")"
      ],
      "metadata": {
        "id": "o8WWDgrp1zfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset, Features, Image, Value, Sequence, DatasetDict\n",
        "\n",
        "def load_coco_annotations(annotation_file, image_dir):\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create image id to annotations mapping\n",
        "    image_annotations = {img['id']: {'image': img, 'annotations': []} for img in data['images']}\n",
        "    for ann in data['annotations']:\n",
        "        image_annotations[ann['image_id']]['annotations'].append(ann)\n",
        "\n",
        "    # Create dataset entries\n",
        "    dataset_entries = []\n",
        "    for img_id, img_data in image_annotations.items():\n",
        "        entry = {\n",
        "            'image_id': img_data['image']['file_name'],\n",
        "            'image': os.path.join(image_dir, img_data['image']['file_name']),\n",
        "            'width': img_data['image']['width'],\n",
        "            'height': img_data['image']['height'],\n",
        "            'annotations': {\n",
        "                'image': img_data['image'],\n",
        "                'categories': {\n",
        "                    'id': [cat['id'] for cat in data['categories']],\n",
        "                    'name': [cat['name'] for cat in data['categories']],\n",
        "                    'supercategory': [cat['supercategory'] for cat in data['categories']]\n",
        "                },\n",
        "                'annotations': {\n",
        "                    'id': [ann['id'] for ann in img_data['annotations']],\n",
        "                    'image_id': [ann['image_id'] for ann in img_data['annotations']],\n",
        "                    'category_id': [ann['category_id'] for ann in img_data['annotations']],\n",
        "                    'bbox': [ann['bbox'] for ann in img_data['annotations']],\n",
        "                    'area': [ann['area'] for ann in img_data['annotations']],\n",
        "                    'segmentation': [ann['segmentation'] for ann in img_data['annotations']],\n",
        "                    'iscrowd': [ann['iscrowd'] for ann in img_data['annotations']]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        dataset_entries.append(entry)\n",
        "\n",
        "    return dataset_entries\n",
        "\n",
        "# Load annotations\n",
        "train_data = load_coco_annotations('/content/coco/train/_annotations.coco.json', '/content/coco/train')\n",
        "test_data = load_coco_annotations('/content/coco/test/_annotations.coco.json', '/content/coco/test')\n",
        "valid_data = load_coco_annotations('/content/coco/valid/_annotations.coco.json', '/content/coco/valid')\n",
        "\n",
        "# Define features\n",
        "features = Features({\n",
        "    'image_id': Value('string'),\n",
        "    'image': Image(),\n",
        "    'width': Value('int32'),\n",
        "    'height': Value('int32'),\n",
        "    'annotations': {\n",
        "        'image': {\n",
        "            'id': Value('int64'),\n",
        "            'license': Value('int64'),\n",
        "            'file_name': Value('string'),\n",
        "            'height': Value('int64'),\n",
        "            'width': Value('int64'),\n",
        "            'date_captured': Value('string')\n",
        "        },\n",
        "        'categories': {\n",
        "            'id': Sequence(Value('int64')),\n",
        "            'name': Sequence(Value('string')),\n",
        "            'supercategory': Sequence(Value('string'))\n",
        "        },\n",
        "        'annotations': {\n",
        "            'id': Sequence(Value('int64')),\n",
        "            'image_id': Sequence(Value('int64')),\n",
        "            'category_id': Sequence(Value('int64')),\n",
        "            'bbox': Sequence(Sequence(Value('float32'), length=4)),\n",
        "            'area': Sequence(Value('float32')),\n",
        "            'segmentation': Sequence(Sequence(Value('float32'))),\n",
        "            'iscrowd': Sequence(Value('int64'))\n",
        "        }\n",
        "    }\n",
        "})\n",
        "\n",
        "# Create datasets\n",
        "coco_train_dataset = Dataset.from_list(train_data, features=features)\n",
        "coco_test_dataset = Dataset.from_list(test_data, features=features)\n",
        "coco_valid_dataset = Dataset.from_list(valid_data, features=features)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "coco_dataset = DatasetDict({\n",
        "    'train': coco_train_dataset,\n",
        "    'test': coco_test_dataset,\n",
        "    'validation': coco_valid_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "BLxgnYl54YXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hub\n",
        "coco_dataset.push_to_hub(\"dwb2023/roboflow100-bccd-coco\")\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(f\"Dataset splits: {coco_dataset.keys()}\")\n",
        "print(f\"Number of training examples: {len(coco_dataset['train'])}\")\n",
        "print(f\"Number of testing examples: {len(coco_dataset['test'])}\")\n",
        "print(f\"Number of validation examples: {len(coco_dataset['validation'])}\")\n",
        "print(f\"Features: {coco_dataset['train'].features}\")\n",
        "\n",
        "# Print a sample entry\n",
        "print(\"\\nSample entry:\")\n",
        "print(coco_dataset['train'][0])"
      ],
      "metadata": {
        "id": "wfHdcr5e95xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset, Features, Image, Value, DatasetDict\n",
        "\n",
        "def load_florence2_annotations(annotation_file, image_dir):\n",
        "    dataset_entries = []\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        for line in f:\n",
        "            ann = json.loads(line)\n",
        "            entry = {\n",
        "                'image_id': ann['image'],\n",
        "                'image': os.path.join(image_dir, ann['image']),\n",
        "                'annotations': ann  # Keep all original fields\n",
        "            }\n",
        "            dataset_entries.append(entry)\n",
        "    return dataset_entries\n",
        "\n",
        "# Load annotations\n",
        "train_data = load_florence2_annotations('/content/florence2_od/train/annotations.jsonl', '/content/florence2_od/train')\n",
        "test_data = load_florence2_annotations('/content/florence2_od/test/annotations.jsonl', '/content/florence2_od/test')\n",
        "valid_data = load_florence2_annotations('/content/florence2_od/valid/annotations.jsonl', '/content/florence2_od/valid')\n",
        "\n",
        "# Define features\n",
        "features = Features({\n",
        "    'image_id': Value('string'),\n",
        "    'image': Image(),\n",
        "    'annotations': {\n",
        "        'image': Value('string'),\n",
        "        'prefix': Value('string'),\n",
        "        'suffix': Value('string')\n",
        "    }\n",
        "})\n",
        "\n",
        "# Create datasets\n",
        "florence2_train_dataset = Dataset.from_list(train_data, features=features)\n",
        "florence2_test_dataset = Dataset.from_list(test_data, features=features)\n",
        "florence2_valid_dataset = Dataset.from_list(valid_data, features=features)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "florence2_dataset = DatasetDict({\n",
        "    'train': florence2_train_dataset,\n",
        "    'test': florence2_test_dataset,\n",
        "    'validation': florence2_valid_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "0r7c4TQq9MyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hub\n",
        "florence2_dataset.push_to_hub(\"dwb2023/roboflow100-bccd-florence2\")\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(f\"Dataset splits: {florence2_dataset.keys()}\")\n",
        "print(f\"Number of training examples: {len(florence2_dataset['train'])}\")\n",
        "print(f\"Number of testing examples: {len(florence2_dataset['test'])}\")\n",
        "print(f\"Number of validation examples: {len(florence2_dataset['validation'])}\")\n",
        "print(f\"Features: {florence2_dataset['train'].features}\")\n",
        "\n",
        "# Print a sample entry\n",
        "print(\"\\nSample entry:\")\n",
        "print(florence2_dataset['train'][0])"
      ],
      "metadata": {
        "id": "TTp5OEYU-K2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAppgYrYDm4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hub\n",
        "paligemma_dataset.push_to_hub(\"dwb2023/roboflow100-bccd-paligemma\")\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(f\"Dataset splits: {paligemma_dataset.keys()}\")\n",
        "print(f\"Number of training examples: {len(paligemma_dataset['train'])}\")\n",
        "print(f\"Number of testing examples: {len(paligemma_dataset['test'])}\")\n",
        "print(f\"Number of validation examples: {len(paligemma_dataset['validation'])}\")\n",
        "print(f\"Features: {paligemma_dataset['train'].features}\")\n",
        "\n",
        "# Print a sample entry\n",
        "print(\"\\nSample entry:\")\n",
        "print(paligemma_dataset['train'][0])"
      ],
      "metadata": {
        "id": "8UfpVbK9-RRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for same image across datasets\n",
        "image_id_to_find = 'BloodImage_00343_jpg.rf.d8c56063ce5e40c50efb00a7e0c83c3b.jpg'\n",
        "\n",
        "# Function to find a record by image_id\n",
        "def find_record_by_image_id(dataset, image_id):\n",
        "    for record in dataset['train']:\n",
        "        if record['image_id'] == image_id:\n",
        "            return record\n",
        "    return None"
      ],
      "metadata": {
        "id": "nSiuyM3kKVro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the COCO record\n",
        "record = find_record_by_image_id(coco_ds, image_id_to_find)\n",
        "\n",
        "# Display the record\n",
        "if record:\n",
        "    print(f\"Record found: {record}\")\n",
        "else:\n",
        "    print(\"Record not found\")"
      ],
      "metadata": {
        "id": "ZPcQ7M6V_z1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the RoboFlow COCO record\n",
        "record = find_record_by_image_id(coco_dataset, image_id_to_find)\n",
        "\n",
        "# Display the record\n",
        "if record:\n",
        "    print(f\"Record found: {record}\")\n",
        "else:\n",
        "    print(\"Record not found\")"
      ],
      "metadata": {
        "id": "5N-AZ9sfNk2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Roboflow Florence2 record\n",
        "record = find_record_by_image_id(florence2_dataset, image_id_to_find)\n",
        "\n",
        "# Display the record\n",
        "if record:\n",
        "    print(f\"Record found: {record}\")\n",
        "else:\n",
        "    print(\"Record not found\")"
      ],
      "metadata": {
        "id": "-gTVMcLgLiqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Roboflow Paligemma record\n",
        "record = find_record_by_image_id(paligemma_dataset, image_id_to_find)\n",
        "\n",
        "# Display the record\n",
        "if record:\n",
        "    print(f\"Record found: {record}\")\n",
        "else:\n",
        "    print(\"Record not found\")"
      ],
      "metadata": {
        "id": "D9v-fIxaNUfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Es23N3qqNcpP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}